{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "jsdQje5qgMj5",
        "MbEAZ5OO8Qx0",
        "ePbt1a3t9sOV",
        "WCMoiK68_glT",
        "HgtpqtN-Aigc",
        "PTxmR34LBcco",
        "YhCtdCbYA1T_",
        "DtSZXSxbBkWq",
        "oeiYOPjVCisk",
        "EcYQwyZKDJSJ",
        "DNkXbreGFwIY",
        "CzHxwHFsIS0A",
        "svF-gNzCItfA"
      ],
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Setting up the environment\n",
        "!pip install evidently googletrans-py\n",
        "!git clone https://github.com/nzmonzmp/dataset-ames.git\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy\n",
        "import pandas\n",
        "import random\n",
        "import requests\n",
        "import scipy.stats\n",
        "import seaborn\n",
        "import sklearn.ensemble\n",
        "import sklearn.feature_extraction.text\n",
        "import sklearn.linear_model\n",
        "import sklearn.model_selection\n",
        "import sklearn.pipeline\n",
        "import warnings\n",
        "import zipfile\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def preprocess(train_file, test_file):\n",
        "  train_X = pandas.read_csv(train_file, index_col=\"Id\")\n",
        "  test_X = pandas.read_csv(test_file, index_col=\"Id\")\n",
        "\n",
        "  train_y = train_X.pop(\"SalePrice\")\n",
        "\n",
        "  all_X = pandas.concat([train_X, test_X])\n",
        "\n",
        "  cols_1 = [\"LotFrontage\"]\n",
        "  all_X[cols_1] = all_X[cols_1].fillna(train_X[cols_1].median())\n",
        "\n",
        "  cols_2 = [\"MSZoning\", \"Electrical\", \"KitchenQual\", \"Exterior1st\",\n",
        "            \"Exterior2nd\", \"SaleType\", \"Utilities\"]\n",
        "  all_X[cols_2] = all_X[cols_2].fillna(train_X[cols_2].mode().iloc[0, :])\n",
        "\n",
        "  cols_4 = [\"GarageYrBlt\", \"GarageArea\", \"GarageCars\", \"BsmtFinSF1\",\n",
        "            \"BsmtFinSF2\", \"BsmtFullBath\", \"BsmtHalfBath\", \"BsmtUnfSF\",\n",
        "            \"MasVnrArea\", \"TotalBsmtSF\"]\n",
        "  all_X[cols_4] = all_X[cols_4].fillna(0)\n",
        "\n",
        "  cols_5 = [\"Functional\"]\n",
        "  all_X[cols_5] = all_X[cols_5].fillna(\"Typ\")\n",
        "\n",
        "  all_X = all_X.fillna(\"NA\")\n",
        "\n",
        "  cols_numerical2label = ['MSSubClass']\n",
        "  all_X[cols_numerical2label] = all_X[cols_numerical2label].astype(str)\n",
        "\n",
        "  quality_mapping = dict(NA=0, Po=1, Fa=2, TA=3, Gd=4, Ex=5)\n",
        "  quality_columns = [\"BsmtCond\", \"BsmtQual\", \"ExterCond\", \"ExterQual\",\n",
        "                      \"FireplaceQu\", \"GarageCond\", \"GarageQual\", \"HeatingQC\",\n",
        "                      \"KitchenQual\", \"PoolQC\"]\n",
        "  street_mapping = dict(NA=0, Grvl=1, Pave=2)\n",
        "  bsmt_fin_mapping = dict(NA=0, Unf=1, LwQ=2, Rec=3, BLQ=4, ALQ=5, GLQ=6)\n",
        "\n",
        "  replace_mapping = dict(\n",
        "    Alley=street_mapping,\n",
        "    BsmtExposure=dict(NA=0, No=1, Mn=2, Av=3, Gd=4),\n",
        "    BsmtFinType1=bsmt_fin_mapping,\n",
        "    BsmtFinType2=bsmt_fin_mapping,\n",
        "    Functional=dict(Sal=1, Sev=2, Maj2=3, Maj1=4, Mod=5, Min2=6, Min1=7, Typ=8),\n",
        "    LandSlope=dict(Sev=1, Mod=2, Gtl=3),\n",
        "    LotShape=dict(IR3=1, IR2=2, IR1=3, Reg=4),\n",
        "    PavedDrive=dict(NA=0, N=1, P=2, Y=3),\n",
        "    Street=dict(Grvl=1, Pave=2),\n",
        "    Utilities=dict(ELO=1, NoSeWa=2, NoSewr=3, AllPub=4),\n",
        "  )\n",
        "\n",
        "  for quality_column in quality_columns:\n",
        "    replace_mapping[quality_column] = quality_mapping\n",
        "\n",
        "  all_X.replace(replace_mapping, inplace=True)\n",
        "\n",
        "  print(f\"Nombre de NAs : {all_X.isnull().sum().sum()}\")\n",
        "\n",
        "  return (all_X.iloc[:train_X.shape[0], :],\n",
        "          train_y,\n",
        "          all_X.iloc[train_X.shape[0]:, :])\n",
        "\n",
        "\n",
        "def download_medicine_reviews() -> tuple[pandas.DataFrame, pandas.DataFrame]:\n",
        "  \"\"\"Data source: https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29\n",
        "\n",
        "  Citation:\n",
        "    Felix Gräßer, Surya Kallumadi, Hagen Malberg, and Sebastian Zaunseder.\n",
        "    2018.\n",
        "    Aspect-Based Sentiment Analysis of Drug Reviews Applying Cross-Domain and Cross-Data Learning.\n",
        "    In Proceedings of the 2018 International Conference on Digital Health (DH '18).\n",
        "    ACM, New York, NY, USA, 121-125.\n",
        "  \"\"\"\n",
        "  content = requests.get(\n",
        "      \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
        "  ).content\n",
        "  with zipfile.ZipFile(io.BytesIO(content)) as arc:\n",
        "      raw_data = pandas.read_csv(arc.open(\"drugsComTest_raw.tsv\"), sep=\"\\t\")\n",
        "  return raw_data[[\"drugName\", \"condition\", \"review\",\t\"rating\"]]\n",
        "\n",
        "\n",
        "def filter_medicine_reviews(df: pandas.DataFrame, condition: str\n",
        "                            ) -> pandas.DataFrame:\n",
        "  df = df.loc[(df[\"condition\"] == condition) & (df[\"rating\"].isin([1, 10])),\n",
        "                     [\"review\", \"rating\"]]\n",
        "  df[\"is_positive\"] = df[\"rating\"].apply(\n",
        "      lambda x: 0 if x == 1 else 1)\n",
        "  return df.drop(columns=\"rating\")\n",
        "\n",
        "\n",
        "def split_medicine_reviews(df: pandas.DataFrame\n",
        "                           ) -> tuple[pandas.DataFrame, pandas.DataFrame]:\n",
        "  X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
        "      df['review'],\n",
        "      df['is_positive'],\n",
        "      test_size=0.4,\n",
        "      random_state=42,\n",
        "      shuffle=True)\n",
        "\n",
        "  reference = pandas.DataFrame({'review': X_train, 'is_positive': y_train})\n",
        "  valid = pandas.DataFrame({'review': X_test, 'is_positive': y_test})\n",
        "  return reference, valid"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R2RCwNzJ4wnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monitoring — Data and Concept Drift"
      ],
      "metadata": {
        "id": "_tB3UgcqybeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started with the `evidently` library"
      ],
      "metadata": {
        "id": "qoNXB8kXm8xB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data"
      ],
      "metadata": {
        "id": "n3r0ZsW7uia9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_df, _, current_df = preprocess(\"dataset-ames/train.csv\", \"dataset-ames/test.csv\")"
      ],
      "metadata": {
        "id": "K-XIRvrh5oDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "630a2437"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from evidently import (\n",
        "    BinaryClassification,\n",
        "    Dataset,\n",
        "    DataDefinition,\n",
        "    Report,\n",
        ")\n",
        "\n",
        "from evidently.descriptors import (\n",
        "    NonLetterCharacterPercentage,\n",
        "    OOVWordsPercentage,\n",
        "    SentenceCount,\n",
        "    TextLength,\n",
        "    WordCount,\n",
        ")\n",
        "\n",
        "from evidently.metrics import MissingValueCount, UniqueValueCount\n",
        "\n",
        "from evidently.presets import (\n",
        "    ClassificationPreset,\n",
        "    DataDriftPreset,\n",
        "    ValueStats,\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "0c00061b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a `Dataset` with a `DataDefinition`"
      ],
      "metadata": {
        "id": "xHyL2z3WjvIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the steps described in the [evidently documentation](https://docs.evidentlyai.com/docs/library/data_definition), create two datasets: one, `reference`, for `reference_df`, and the other, `current`, for `current_df`. You can use the default `DataDefinition`."
      ],
      "metadata": {
        "id": "mJTctQXKZzwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "41KD63zwjuVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "jsdQje5qgMj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference = Dataset.from_pandas(reference_df, data_definition=DataDefinition())\n",
        "current = Dataset.from_pandas(current_df, data_definition=DataDefinition())"
      ],
      "metadata": {
        "id": "ZX4YQyLhgNuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reports"
      ],
      "metadata": {
        "id": "fedb4612"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evidently can generate reports and test suites.\n",
        "\n",
        "Reports are meant to be read and studied by humans, whereas test suites are more intended for automation, for example to trigger automatic retraining.\n",
        "\n",
        "Start by creating a very first report that uses the [default parameters to detect data drift](https://docs.evidentlyai.com/metrics/preset_data_drift)."
      ],
      "metadata": {
        "id": "UVwQWsod7oo1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Your code here"
      ],
      "outputs": [],
      "metadata": {
        "id": "bb77cbe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "MbEAZ5OO8Qx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report = Report([DataDriftPreset()])\n",
        "run = report.run(current_data=current, reference_data=reference)\n",
        "run"
      ],
      "metadata": {
        "id": "fau_CZuJ8SB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting specific columns"
      ],
      "metadata": {
        "id": "y9GfDipd8UBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dataset we use for these hands-on exercises (AMES), two columns are particularly important: `OverallQual` and `GrLivArea`.\n",
        "\n",
        "Create a report on these two columns with the `UniqueValueCount` metric for the `OverallQual` column and the `ValueStats` preset for the `GrLivArea` column."
      ],
      "metadata": {
        "id": "9DHmCJ-q8guf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "3M5qhVxg8qcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "ePbt1a3t9sOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "report = Report([UniqueValueCount(column=\"OverallQual\"), ValueStats(\"GrLivArea\")])\n",
        "run = report.run(current_data=current, reference_data=reference)\n",
        "run"
      ],
      "outputs": [],
      "metadata": {
        "id": "7e7de377"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving a report"
      ],
      "metadata": {
        "id": "Nqw_-Uat_Gfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can [save a report](https://docs.evidentlyai.com/docs/library/output_formats) in HTML format to read it directly or in JSON format to be consumed by other programs.\n",
        "\n",
        "Save the data drift report in JSON format."
      ],
      "metadata": {
        "id": "DVyLWWj9_Iu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "EZCuWg7d_fql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "WCMoiK68_glT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report = Report([DataDriftPreset()])\n",
        "eval = report.run(reference_data=reference, current_data=current)\n",
        "eval.save_json('data-drift-report.json')"
      ],
      "metadata": {
        "id": "6RYXLw0Q_kMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test suites"
      ],
      "metadata": {
        "id": "1ea31ae7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Test suites](https://docs.evidentlyai.com/docs/library/tests) are better suited than reports for an automated context such as CI/CD.\n",
        "\n",
        "To start, create a first test suite that uses the `DataDriftPreset` preset that we used above to produce a report."
      ],
      "metadata": {
        "id": "CVQnwfylAL_H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Your code here"
      ],
      "outputs": [],
      "metadata": {
        "id": "f2311155"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "HgtpqtN-Aigc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report = Report([DataDriftPreset()], include_tests=True)\n",
        "run = report.run(reference_data=reference, current_data=current)\n",
        "run"
      ],
      "metadata": {
        "id": "DAd16Oc2Ajuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing results in Python"
      ],
      "metadata": {
        "id": "Fcwr31VUBP3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since tests are more often used for automation, we regularly analyze their results in Python. Compute the percentage of successful tests from the latest test suite. You can use the `dict` method."
      ],
      "metadata": {
        "id": "hS6J0cPdBRYs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Your code here"
      ],
      "outputs": [],
      "metadata": {
        "id": "5fedd579"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "PTxmR34LBcco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = run.dict()\n",
        "successes = sum(d[\"status\"] == \"SUCCESS\" for d in results[\"tests\"])\n",
        "percentage = successes / len(results[\"tests\"]) * 100\n",
        "percentage"
      ],
      "metadata": {
        "id": "CF-aoPF5BeL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying the `evidently` library to text data"
      ],
      "metadata": {
        "id": "Cyv8wMPMm3In"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data"
      ],
      "metadata": {
        "id": "qVcOsoGQK4PJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data we will use are reviews about medications used to treat several conditions. For now, we will focus on medications used to treat pain, *Pain* in the data."
      ],
      "metadata": {
        "id": "mukd-AcEK9sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = download_medicine_reviews()\n",
        "filtered_data = filter_medicine_reviews(raw_data, \"Pain\")\n",
        "reference_df, current_df = split_medicine_reviews(filtered_data)"
      ],
      "metadata": {
        "id": "wgt0CmMj1C3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_df"
      ],
      "metadata": {
        "id": "0TQPDqoQ1uaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a classification model"
      ],
      "metadata": {
        "id": "U0SwHiH54h3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = sklearn.pipeline.Pipeline(\n",
        "    [\n",
        "        (\"vectorization\",\n",
        "         sklearn.feature_extraction.text.TfidfVectorizer(\n",
        "             sublinear_tf=True,\n",
        "             max_df=0.5,\n",
        "             stop_words=\"english\")),\n",
        "        (\"classification\",\n",
        "         sklearn.linear_model.SGDClassifier(\n",
        "             alpha=0.0001,\n",
        "             max_iter=50,\n",
        "             penalty='l1',\n",
        "             loss='modified_huber',\n",
        "             random_state=42))\n",
        "    ])\n",
        "pipeline.fit(reference_df['review'].values, reference_df['is_positive'].values)"
      ],
      "metadata": {
        "id": "Dr9Qlun_7LqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new column in `reference_df` and `current_df` that contains the predictions:"
      ],
      "metadata": {
        "id": "-d7X8PsDO7fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_df['predictions'] = pipeline.predict(reference_df['review'].values)\n",
        "current_df['predictions'] = pipeline.predict(current_df['review'].values)"
      ],
      "metadata": {
        "id": "1Kbopctq7k1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_df"
      ],
      "metadata": {
        "id": "AQXbCoWmg7xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define two `Dataset` objects (`reference` and `current`) using a `DataDefinition` adapted to the three columns `review`, `is_positive`, and `predictions` in our data."
      ],
      "metadata": {
        "id": "avmKhPjPAZ-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "ZINp51xGBAFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "YhCtdCbYA1T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_definition = DataDefinition(\n",
        "    text_columns=[\"review\"],\n",
        "    classification=[BinaryClassification(target=\"is_positive\",\n",
        "                                         prediction_labels=\"predictions\")]\n",
        ")\n",
        "\n",
        "reference = Dataset.from_pandas(reference_df, data_definition=data_definition)\n",
        "current = Dataset.from_pandas(current_df, data_definition=data_definition)"
      ],
      "metadata": {
        "id": "61ZOAQw58q4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification quality report"
      ],
      "metadata": {
        "id": "W61fnmwLA4DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a [classification quality](https://docs.evidentlyai.com/metrics/preset_classification) report from the reference and validation data."
      ],
      "metadata": {
        "id": "xExX1rHPPgpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "HzUFybyGBjKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "DtSZXSxbBkWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report = Report([ClassificationPreset()])\n",
        "\n",
        "run = report.run(reference_data=reference, current_data=current)\n",
        "run"
      ],
      "metadata": {
        "id": "n5PqmKOw85r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detecting a “technical” data drift"
      ],
      "metadata": {
        "id": "69kg-dCWBoCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to simulate a common event in a processing pipeline: a bug leads to poor-quality processing. Here, we will even simulate two:\n",
        "\n",
        "- Bug in the cleaning of HTML tags during review preprocessing\n",
        "- Data ingestion bug causing the data to be in a different language than the training language"
      ],
      "metadata": {
        "id": "CxXpm1AgQ_xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_str(s):\n",
        "  return translator.translate(s, dest='fr').text\n",
        "\n",
        "random_html_tags = ('<body>, </body>', '<html><body>', '</body></html>', '<h1>', '</h1>',\n",
        "                    '<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 0 0\" width=\"0\" height=\"0\" focusable=\"false\" role=\"none\" style=\"visibility: hidden; position: absolute; left: -9999px; overflow: hidden;\"><defs><filter id=\"wp-duotone-magenta-yellow\"><feColorMatrix color-interpolation-filters=\"sRGB\" type=\"matrix\" values=\" .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 .299 .587 .114 0 0 \"></feColorMatrix><feComponentTransfer color-interpolation-filters=\"sRGB\"><feFuncR type=\"table\" tableValues=\"0.78039215686275 1\"></feFuncR><feFuncG type=\"table\" tableValues=\"0 0.94901960784314\"></feFuncG><feFuncB type=\"table\" tableValues=\"0.35294117647059 0.47058823529412\"></feFuncB><feFuncA type=\"table\" tableValues=\"1 1\"></feFuncA></feComponentTransfer><feComposite in2=\"SourceGraphic\" operator=\"in\"></feComposite></filter></defs></svg>')\n",
        "\n",
        "def inject_random_html_tags(s):\n",
        "  num_tags = 25\n",
        "  for i in range(num_tags):\n",
        "    random.seed(i)\n",
        "    pos = random.choice(range(len(s)))\n",
        "    s = s[:pos] + random.choice(random_html_tags) + s[pos:]\n",
        "\n",
        "  return s"
      ],
      "metadata": {
        "id": "ys8RG24FTuAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_disturbed_df = current_df[['review', 'is_positive']].copy()"
      ],
      "metadata": {
        "id": "Mierji-ZOYhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disturbed_num = int(len(current_disturbed_df) * 0.5)\n",
        "random.seed(42)\n",
        "disturbed_ind = random.sample(list(current_disturbed_df.index), k=disturbed_num)\n",
        "current_disturbed_df.loc[disturbed_ind[:int(disturbed_num / 10)], 'review'] = \\\n",
        "current_disturbed_df.loc[disturbed_ind[:int(disturbed_num / 10)], 'review'].apply(inject_random_html_tags)\n",
        "# current_disturbed_df.loc[disturbed_ind[int(disturbed_num / 10):], 'review'] = \\\n",
        "# current_disturbed_df.loc[disturbed_ind[int(disturbed_num / 10):], 'review'].apply(translate_str)"
      ],
      "metadata": {
        "id": "lJqzSPg_O8nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_disturbed_df['predictions'] = pipeline.predict(current_disturbed_df['review'].values)\n",
        "current_disturbed = Dataset.from_pandas(current_disturbed_df, data_definition=data_definition)"
      ],
      "metadata": {
        "id": "sTaent8yp_3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Producing a new quality report"
      ],
      "metadata": {
        "id": "ypmwZJeqCOgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reuse the previous code to analyze the model performance on this degraded data, comparing this time against the “clean” validation data."
      ],
      "metadata": {
        "id": "OS8783aCCSVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "-jRA6ZEzChmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "oeiYOPjVCisk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report = Report([ClassificationPreset()])\n",
        "\n",
        "run = report.run(reference_data=current, current_data=current_disturbed)\n",
        "run"
      ],
      "metadata": {
        "id": "7MeMiL0KUf7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing the model’s poor performance"
      ],
      "metadata": {
        "id": "uAS-Y-zjC6oQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Produce a data drift report that shows the drift of the `is_positive` and `predictions` columns, as well as the drift of text [descriptors](https://docs.evidentlyai.com/docs/library/descriptors) for the `review` column. You will first need to define a list of text descriptors to use and add them to the datasets already defined using the `add_descriptors` method."
      ],
      "metadata": {
        "id": "bXkI11E5C96o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "yXWI5SeoSKME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "EcYQwyZKDJSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "descriptors = [\n",
        "    NonLetterCharacterPercentage(\"review\", alias=\"non_letters\"),\n",
        "    OOVWordsPercentage(\"review\", alias=\"oov\"),\n",
        "    SentenceCount(\"review\", alias=\"sentence_count\"),\n",
        "    TextLength(\"review\", alias=\"text_length\"),\n",
        "    WordCount(\"review\", alias=\"word_count\"),\n",
        "]\n",
        "\n",
        "reference.add_descriptors(descriptors)\n",
        "current.add_descriptors(descriptors)\n",
        "current_disturbed.add_descriptors(descriptors)\n",
        "\n",
        "report = Report([DataDriftPreset()])\n",
        "\n",
        "run = report.run(reference_data=current, current_data=current_disturbed)\n",
        "run"
      ],
      "metadata": {
        "id": "RJSXCrsvRFCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual inspection of faulty examples"
      ],
      "metadata": {
        "id": "uCwR9d0vD214"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe an increase in long texts and in the presence of out-of-vocabulary (*OOV*) words.\n",
        "\n",
        "Use the export of descriptors as a dataframe to inspect faulty examples, for instance:\n",
        "\n",
        "- Reviews with a length greater than 1000 characters\n",
        "- Reviews with more than 30% out-of-vocabulary words. How does `evidently` define the vocabulary?"
      ],
      "metadata": {
        "id": "Kl4ghH4ZD6Kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "yQMWi6mMFvCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "DNkXbreGFwIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "descriptors_df = current_disturbed.as_dataframe()\n",
        "descriptors_df"
      ],
      "metadata": {
        "id": "u72x-JgMauJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "descriptors_df[descriptors_df['text_length'] > 1000]"
      ],
      "metadata": {
        "id": "f7IigmqUbUQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "descriptors_df[descriptors_df['oov'] > 30]"
      ],
      "metadata": {
        "id": "edpfz1JAYlPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data drift"
      ],
      "metadata": {
        "id": "VePLsS-P-s3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s now simulate data drift by looking at another part of our original data: reviews about medications used to treat depression."
      ],
      "metadata": {
        "id": "n2n9eSx2akjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_content_df = filter_medicine_reviews(raw_data, \"Depression\")\n",
        "new_content_df"
      ],
      "metadata": {
        "id": "6SJlPB80-4bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_content_df[\"predictions\"] = pipeline.predict(new_content_df.review.values)"
      ],
      "metadata": {
        "id": "f6Fj7emu_U2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification quality report"
      ],
      "metadata": {
        "id": "gE3GZ6FPIGI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, use `evidently` to quantify how the model performance evolves."
      ],
      "metadata": {
        "id": "fwXwyHLgIKpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "1zZCVmUmIRp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "CzHxwHFsIS0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_content = Dataset.from_pandas(new_content_df,\n",
        "                                  data_definition=data_definition,\n",
        "                                  descriptors=descriptors)\n",
        "\n",
        "report = Report([ClassificationPreset()])\n",
        "\n",
        "run = report.run(reference_data=current, current_data=new_content)\n",
        "run"
      ],
      "metadata": {
        "id": "5gJkUeAT_jeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detecting data drift"
      ],
      "metadata": {
        "id": "dk_VicNUIWXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsurprisingly, performance is very degraded. Produce a data drift report. Would `evidently` have detected this drift in time to allow retraining?"
      ],
      "metadata": {
        "id": "Q6aBtKylIaAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "pJfe3oP2Ir1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "svF-gNzCItfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report = Report([DataDriftPreset()])\n",
        "\n",
        "run = report.run(reference_data=current, current_data=new_content)\n",
        "run"
      ],
      "metadata": {
        "id": "-B6zOYVvgta0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}